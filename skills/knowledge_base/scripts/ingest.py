import os
import sys
import glob

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path ä»¥å¯¼å…¥ agent_core
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_DIR)))
sys.path.append(PROJECT_ROOT)

from agent_core.tools import read_file
from skills.knowledge_base.scripts.db_manager import DBManager

def chunk_text_by_lines(text, chunk_size=20, overlap=5):
    """
    æŒ‰è¡Œåˆ‡åˆ†æ–‡æœ¬ï¼Œå¹¶å°è¯•æå–è¯­ä¹‰åŒ–çš„ä½ç½®ä¿¡æ¯ï¼ˆå¦‚ Slide 1, Page 2, Sheet Nameï¼‰ã€‚
    è¿”å›: List[dict] -> [{'text': '...', 'lines': '10-30', 'location': 'Slide 5'}]
    """
    lines = text.splitlines()
    chunks = []
    total_lines = len(lines)
    
    # é¢„æ‰«æï¼šå»ºç«‹è¡Œå·åˆ°ä½ç½®çš„æ˜ å°„
    # line_location_map[line_index] = "Slide 1"
    line_location_map = {}
    current_location = "Unknown Location"
    
    import re
    # åŒ¹é…æ¨¡å¼: --- Slide 1 ---, --- Page 1 ---, --- Sheet: Sheet1 ---
    loc_pattern = re.compile(r'^--- (Slide \d+|Page \d+|Sheet: .+) ---$')
    
    for i, line in enumerate(lines):
        match = loc_pattern.match(line.strip())
        if match:
            current_location = match.group(1)
        line_location_map[i] = current_location
    
    for i in range(0, total_lines, chunk_size - overlap):
        end = min(i + chunk_size, total_lines)
        chunk_lines = lines[i:end]
        chunk_content = "\n".join(chunk_lines).strip()
        
        if not chunk_content: continue
        
        # è·å–å½“å‰ Chunk å¯¹åº”çš„ä¸»è¦ä½ç½®ï¼ˆå–ä¸­é—´è¡Œçš„ä½ç½®ï¼Œæˆ–è€…èµ·å§‹è¡Œçš„ä½ç½®ï¼‰
        # å–èµ·å§‹è¡Œçš„ä½ç½®é€šå¸¸æ¯”è¾ƒå‡†ï¼Œå› ä¸º Context è¦†ç›–äº†åæ–‡
        # ä½†å¦‚æœ Chunk è·¨é¡µäº†æ€ä¹ˆåŠï¼Ÿ
        # æˆ‘ä»¬å¯ä»¥è®°å½• rangeï¼Œä¾‹å¦‚ "Slide 1 - Slide 2"
        start_loc = line_location_map.get(i, "Unknown")
        end_loc = line_location_map.get(end-1, "Unknown")
        
        if start_loc == end_loc:
            location = start_loc
        else:
            location = f"{start_loc} -> {end_loc}"
            
        chunks.append({
            "text": chunk_content,
            "line_start": i + 1,
            "line_end": end,
            "location": location
        })
        
        if end == total_lines: break
        
    return chunks

def ingest_file(file_path, collection_name="documents"):
    # ... (å‰æ–‡è¯»å–é€»è¾‘ä¿æŒä¸å˜)
    # æ—¢ç„¶æˆ‘ä»¬è¦ä¿®æ”¹ chunking é€»è¾‘ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ ingest_file çš„ååŠéƒ¨åˆ†ä¹Ÿæ›¿æ¢æ‰
    # ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘å°†æ›¿æ¢æ•´ä¸ª ingest_file å‡½æ•°çš„ååŠéƒ¨åˆ†
    pass


def ingest_file(file_path, collection_name="documents"):
    print(f"ğŸ“„ Processing: {file_path}")
    
    # 1. è°ƒç”¨ Core Tool è¯»å–æ–‡ä»¶ (åˆ©ç”¨å…¶å¼ºå¤§çš„è§£æèƒ½åŠ›)
    # ä¸ä½¿ç”¨ outline_onlyï¼Œç›´æ¥è¯»å…¨æ–‡ (åˆ©ç”¨æ–°ç‰¹æ€§: end_line=-1)
    # æ³¨æ„ï¼šread_file å†…éƒ¨æœ‰æˆªæ–­ä¿æŠ¤ï¼Œä½†æˆ‘ä»¬ä½œä¸ºå†…éƒ¨è°ƒç”¨ï¼Œå¸Œæœ›è¯»å…¨é‡ã€‚
    # æˆ‘ä»¬éœ€è¦ç»•è¿‡ read_file çš„ 500 è¡Œä¿æŠ¤å—ï¼Ÿ
    # æ˜¯çš„ã€‚ä½† read_file çš„å®ç°æ˜¯ end_line=-1 æ—¶é»˜è®¤æˆªæ–­ã€‚
    # æˆ‘ä»¬å¯ä»¥ loop è¯»å–ï¼Œæˆ–è€…ä¿®æ”¹ read_file çš„é€»è¾‘ã€‚
    # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆè¯»å‰ 2000 è¡Œã€‚å¦‚æœæ–‡ä»¶è¶…å¤§ï¼ŒIngest è„šæœ¬åº”è¯¥å®ç°åˆ†é¡µå¾ªç¯ã€‚
    
    full_content = ""
    start_line = 1
    page_size = 1000 # æ¯æ¬¡è¯» 1000 è¡Œ
    
    while True:
        # è°ƒç”¨ tool.invoke æˆ–è€…æ˜¯ç›´æ¥å¯¼å…¥å‡½æ•°è°ƒç”¨
        # è¿™é‡Œç›´æ¥è°ƒç”¨å‡½æ•°ï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ python è„šæœ¬é‡Œï¼‰
        # ä½† read_file æ˜¯ StructuredToolï¼Œéœ€è¦ .invoke æˆ– .func
        # ç®€å•èµ·è§ï¼Œç›´æ¥è°ƒç”¨åº•å±‚çš„ _read_docx ç­‰ï¼Ÿä¸ï¼Œé‚£æ ·ç ´åäº†å°è£…ã€‚
        # æˆ‘ä»¬ç”¨ read_file.func
        
        part = read_file.func(file_path, start_line=start_line, end_line=start_line + page_size)
        
        # å»é™¤ Header/Footer å™ªéŸ³
        # è¿™æ˜¯ä¸€ä¸ª hackï¼Œä½†æœ‰æ•ˆ
        body = part
        if "--- æ–‡ä»¶å…ƒæ•°æ® ---" in part:
            body = part.split("--- æ–‡ä»¶å…ƒæ•°æ® ---")[1].split("\n", 4)[-1] # è·³è¿‡å¤´å‡ è¡Œ
        if "[SYSTEM WARNING]" in body:
            body = body.split("[SYSTEM WARNING]")[0]
            
        full_content += body
        
        # æ£€æŸ¥æ˜¯å¦è¯»å®Œ
        if "[SYSTEM WARNING]" not in part: 
            break
        start_line += page_size
        if start_line > 10000: # å®‰å…¨ç†”æ–­
            print("âš ï¸ File too large (>10k lines), stopping.")
            break

    # 2. åˆ‡ç‰‡
    chunks = chunk_text_by_lines(full_content)
    print(f"   -> Split into {len(chunks)} chunks.")
    
    if not chunks: return

    # 3. å‘é‡åŒ– & å­˜å‚¨
    db = DBManager.get_instance()
    vectors = db.embed_documents([c['text'] for c in chunks])
    
    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿ Agent åç»­è¯»å–
    try:
        rel_path = os.path.relpath(file_path, PROJECT_ROOT)
    except ValueError:
        # å¦‚æœè·¨ç›˜ç¬¦ï¼ˆWindowsï¼‰æˆ–è·¯å¾„å¼‚å¸¸ï¼Œå›é€€åˆ°åŸå§‹è·¯å¾„
        rel_path = file_path

    data = []
    for i, chunk in enumerate(chunks):
        data.append({
            "vector": vectors[i],
            "text": chunk['text'],
            "source": rel_path, # [ä¿®æ”¹] å­˜å‚¨ç›¸å¯¹è·¯å¾„
            "line_range": f"{chunk['line_start']}-{chunk['line_end']}",
            "location": chunk['location'], 
            "type": "document"
        })

        
    # 4. å†™å…¥ DB
    # æ£€æŸ¥ Schema å…¼å®¹æ€§
    is_compatible = db.check_schema_compatibility(collection_name, data[0])
    
    tbl = db.get_table(collection_name)
    if tbl and is_compatible:
        tbl.add(data)
    else:
        # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œæˆ–è€…åˆšæ‰å› ä¸ºä¸å…¼å®¹è¢«åˆ é™¤äº†ï¼Œè¿™é‡Œä¼šåˆ›å»ºæ–°è¡¨
        db.create_table(collection_name, data)
        
    print(f"âœ… Ingested {len(data)} vectors to '{collection_name}'.")

def main(input_path, collection="documents"):
    if os.path.isfile(input_path):
        ingest_file(input_path, collection)
    elif os.path.isdir(input_path):
        # é€’å½’æŸ¥æ‰¾æ”¯æŒçš„æ ¼å¼
        exts = ['*.docx', '*.pdf', '*.xlsx', '*.pptx', '*.md', '*.txt']
        files = []
        for ext in exts:
            files.extend(glob.glob(os.path.join(input_path, '**', ext), recursive=True))
            
        print(f"ğŸ” Found {len(files)} files in {input_path}")
        for f in files:
            try:
                ingest_file(f, collection)
            except Exception as e:
                print(f"âŒ Error processing {f}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python ingest.py <file_or_dir> [collection_name]")
        sys.exit(1)
    
    target = sys.argv[1]
    coll = sys.argv[2] if len(sys.argv) > 2 else "documents"
    main(target, coll)
